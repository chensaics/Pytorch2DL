{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from rich import print\n",
    "import torch.utils.benchmark as benchmark\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows和Linux上使用GPU\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Mac 上使用 GPU加速：\n",
    "# device = torch.device(\"mps\")\n",
    "device = \"mps\" if torch.backends.mps.is_built() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置超参数：\n",
    "batch_size = 32\n",
    "max_sequence_len = 128\n",
    "\n",
    "num_heads = 8\n",
    "heads_per_dim = 64\n",
    "embed_dimension = num_heads * heads_per_dim\n",
    "block_size = 1024\n",
    "dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计时器:\n",
    "def torch_timer(f, *args, **kwargs):\n",
    "    t0 = benchmark.Timer(\n",
    "        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}\n",
    "    )\n",
    "    return t0.blocked_autorange().mean * 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads: int, embed_dimension: int, bias: bool=False, dropout:float=0.0):\n",
    "        super().__init__()\n",
    "        assert embed_dimension % num_heads == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(embed_dimension, 3 * embed_dimension, bias=bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(embed_dimension, embed_dimension, bias=bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dimension = embed_dimension\n",
    "        self.dropout = dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size))\n",
    "                                        .view(1, 1, block_size, block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (embed_dimension)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.embed_dimension, dim=2)\n",
    "        k = k.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CausalSelfAttention</span><span style=\"font-weight: bold\">(</span>\n",
       "  <span style=\"font-weight: bold\">(</span>c_attn<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1536</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">(</span>c_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">(</span>attn_dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">(</span>resid_dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mCausalSelfAttention\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mc_attn\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mc_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mattn_dropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mresid_dropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = CausalSelfAttention(num_heads=num_heads, \n",
    "                            embed_dimension=embed_dimension, \n",
    "                            bias=False, \n",
    "                            dropout=0.1).to(\"mps\").to(dtype).eval() # mps / cuda\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">原始model 运行时间： <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9169.492</span> microseconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "原始model 运行时间： \u001b[1;36m9169.492\u001b[0m microseconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 模拟数据\n",
    "x = torch.rand(batch_size,\n",
    "               max_sequence_len,\n",
    "               embed_dimension,\n",
    "               device=device, \n",
    "               dtype=dtype)\n",
    "\n",
    "print(f\"原始model 运行时间： {torch_timer(model, x):.3f} microseconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-03-19 15:01:54,160] torch._dynamo.convert_frame: [ERROR] WON'T CONVERT forward /var/folders/1_/3zjkdvv134x2rw13wbvrh2vh0000gn/T/ipykernel_11159/617640278.py line 24 \n",
      " 25           0 LOAD_FAST                1 (x)\n",
      "              2 LOAD_METHOD              0 (size)\n",
      "              4 CALL_METHOD              0\n",
      "              6 UNPACK_SEQUENCE          3\n",
      "              8 STORE_FAST               2 (B)\n",
      "             10 STORE_FAST               3 (T)\n",
      "             12 STORE_FAST               4 (C)\n",
      "\n",
      " 28          14 LOAD_FAST                0 (self)\n",
      "             16 LOAD_METHOD              1 (c_attn)\n",
      "             18 LOAD_FAST                1 (x)\n",
      "             20 CALL_METHOD              1\n",
      "             22 LOAD_ATTR                2 (split)\n",
      "             24 LOAD_FAST                0 (self)\n",
      "             26 LOAD_ATTR                3 (embed_dimension)\n",
      "             28 LOAD_CONST               1 (2)\n",
      "             30 LOAD_CONST               2 (('dim',))\n",
      "             32 CALL_FUNCTION_KW         2\n",
      "             34 UNPACK_SEQUENCE          3\n",
      "             36 STORE_FAST               5 (q)\n",
      "             38 STORE_FAST               6 (k)\n",
      "             40 STORE_FAST               7 (v)\n",
      "\n",
      " 29          42 LOAD_FAST                6 (k)\n",
      "             44 LOAD_METHOD              4 (view)\n",
      "             46 LOAD_FAST                2 (B)\n",
      "             48 LOAD_FAST                3 (T)\n",
      "             50 LOAD_FAST                0 (self)\n",
      "             52 LOAD_ATTR                5 (num_heads)\n",
      "             54 LOAD_FAST                4 (C)\n",
      "             56 LOAD_FAST                0 (self)\n",
      "             58 LOAD_ATTR                5 (num_heads)\n",
      "             60 BINARY_FLOOR_DIVIDE\n",
      "             62 CALL_METHOD              4\n",
      "             64 LOAD_METHOD              6 (transpose)\n",
      "             66 LOAD_CONST               3 (1)\n",
      "             68 LOAD_CONST               1 (2)\n",
      "             70 CALL_METHOD              2\n",
      "             72 STORE_FAST               6 (k)\n",
      "\n",
      " 30          74 LOAD_FAST                5 (q)\n",
      "             76 LOAD_METHOD              4 (view)\n",
      "             78 LOAD_FAST                2 (B)\n",
      "             80 LOAD_FAST                3 (T)\n",
      "             82 LOAD_FAST                0 (self)\n",
      "             84 LOAD_ATTR                5 (num_heads)\n",
      "             86 LOAD_FAST                4 (C)\n",
      "             88 LOAD_FAST                0 (self)\n",
      "             90 LOAD_ATTR                5 (num_heads)\n",
      "             92 BINARY_FLOOR_DIVIDE\n",
      "             94 CALL_METHOD              4\n",
      "             96 LOAD_METHOD              6 (transpose)\n",
      "             98 LOAD_CONST               3 (1)\n",
      "            100 LOAD_CONST               1 (2)\n",
      "            102 CALL_METHOD              2\n",
      "            104 STORE_FAST               5 (q)\n",
      "\n",
      " 31         106 LOAD_FAST                7 (v)\n",
      "            108 LOAD_METHOD              4 (view)\n",
      "            110 LOAD_FAST                2 (B)\n",
      "            112 LOAD_FAST                3 (T)\n",
      "            114 LOAD_FAST                0 (self)\n",
      "            116 LOAD_ATTR                5 (num_heads)\n",
      "            118 LOAD_FAST                4 (C)\n",
      "            120 LOAD_FAST                0 (self)\n",
      "            122 LOAD_ATTR                5 (num_heads)\n",
      "            124 BINARY_FLOOR_DIVIDE\n",
      "            126 CALL_METHOD              4\n",
      "            128 LOAD_METHOD              6 (transpose)\n",
      "            130 LOAD_CONST               3 (1)\n",
      "            132 LOAD_CONST               1 (2)\n",
      "            134 CALL_METHOD              2\n",
      "            136 STORE_FAST               7 (v)\n",
      "\n",
      " 34         138 LOAD_FAST                0 (self)\n",
      "            140 LOAD_ATTR                7 (flash)\n",
      "            142 POP_JUMP_IF_FALSE       85 (to 170)\n",
      "\n",
      " 36         144 LOAD_GLOBAL              8 (F)\n",
      "            146 LOAD_ATTR                9 (scaled_dot_product_attention)\n",
      "            148 LOAD_FAST                5 (q)\n",
      "            150 LOAD_FAST                6 (k)\n",
      "            152 LOAD_FAST                7 (v)\n",
      "            154 LOAD_CONST               0 (None)\n",
      "            156 LOAD_FAST                0 (self)\n",
      "            158 LOAD_ATTR               10 (dropout)\n",
      "            160 LOAD_CONST               4 (True)\n",
      "            162 LOAD_CONST               5 (('attn_mask', 'dropout_p', 'is_causal'))\n",
      "            164 CALL_FUNCTION_KW         6\n",
      "            166 STORE_FAST               8 (y)\n",
      "            168 JUMP_FORWARD            59 (to 288)\n",
      "\n",
      " 39     >>  170 LOAD_FAST                5 (q)\n",
      "            172 LOAD_FAST                6 (k)\n",
      "            174 LOAD_METHOD              6 (transpose)\n",
      "            176 LOAD_CONST               6 (-2)\n",
      "            178 LOAD_CONST               7 (-1)\n",
      "            180 CALL_METHOD              2\n",
      "            182 BINARY_MATRIX_MULTIPLY\n",
      "            184 LOAD_CONST               8 (1.0)\n",
      "            186 LOAD_GLOBAL             11 (math)\n",
      "            188 LOAD_METHOD             12 (sqrt)\n",
      "            190 LOAD_FAST                6 (k)\n",
      "            192 LOAD_METHOD              0 (size)\n",
      "            194 LOAD_CONST               7 (-1)\n",
      "            196 CALL_METHOD              1\n",
      "            198 CALL_METHOD              1\n",
      "            200 BINARY_TRUE_DIVIDE\n",
      "            202 BINARY_MULTIPLY\n",
      "            204 STORE_FAST               9 (att)\n",
      "\n",
      " 40         206 LOAD_FAST                9 (att)\n",
      "            208 LOAD_METHOD             13 (masked_fill)\n",
      "            210 LOAD_FAST                0 (self)\n",
      "            212 LOAD_ATTR               14 (bias)\n",
      "            214 LOAD_CONST               0 (None)\n",
      "            216 LOAD_CONST               0 (None)\n",
      "            218 BUILD_SLICE              2\n",
      "            220 LOAD_CONST               0 (None)\n",
      "            222 LOAD_CONST               0 (None)\n",
      "            224 BUILD_SLICE              2\n",
      "            226 LOAD_CONST               0 (None)\n",
      "            228 LOAD_FAST                3 (T)\n",
      "            230 BUILD_SLICE              2\n",
      "            232 LOAD_CONST               0 (None)\n",
      "            234 LOAD_FAST                3 (T)\n",
      "            236 BUILD_SLICE              2\n",
      "            238 BUILD_TUPLE              4\n",
      "            240 BINARY_SUBSCR\n",
      "            242 LOAD_CONST               9 (0)\n",
      "            244 COMPARE_OP               2 (==)\n",
      "            246 LOAD_GLOBAL             15 (float)\n",
      "            248 LOAD_CONST              10 ('-inf')\n",
      "            250 CALL_FUNCTION            1\n",
      "            252 CALL_METHOD              2\n",
      "            254 STORE_FAST               9 (att)\n",
      "\n",
      " 41         256 LOAD_GLOBAL              8 (F)\n",
      "            258 LOAD_ATTR               16 (softmax)\n",
      "            260 LOAD_FAST                9 (att)\n",
      "            262 LOAD_CONST               7 (-1)\n",
      "            264 LOAD_CONST               2 (('dim',))\n",
      "            266 CALL_FUNCTION_KW         2\n",
      "            268 STORE_FAST               9 (att)\n",
      "\n",
      " 42         270 LOAD_FAST                0 (self)\n",
      "            272 LOAD_METHOD             17 (attn_dropout)\n",
      "            274 LOAD_FAST                9 (att)\n",
      "            276 CALL_METHOD              1\n",
      "            278 STORE_FAST               9 (att)\n",
      "\n",
      " 43         280 LOAD_FAST                9 (att)\n",
      "            282 LOAD_FAST                7 (v)\n",
      "            284 BINARY_MATRIX_MULTIPLY\n",
      "            286 STORE_FAST               8 (y)\n",
      "\n",
      " 44     >>  288 LOAD_FAST                8 (y)\n",
      "            290 LOAD_METHOD              6 (transpose)\n",
      "            292 LOAD_CONST               3 (1)\n",
      "            294 LOAD_CONST               1 (2)\n",
      "            296 CALL_METHOD              2\n",
      "            298 LOAD_METHOD             18 (contiguous)\n",
      "            300 CALL_METHOD              0\n",
      "            302 LOAD_METHOD              4 (view)\n",
      "            304 LOAD_FAST                2 (B)\n",
      "            306 LOAD_FAST                3 (T)\n",
      "            308 LOAD_FAST                4 (C)\n",
      "            310 CALL_METHOD              3\n",
      "            312 STORE_FAST               8 (y)\n",
      "\n",
      " 47         314 LOAD_FAST                0 (self)\n",
      "            316 LOAD_METHOD             19 (resid_dropout)\n",
      "            318 LOAD_FAST                0 (self)\n",
      "            320 LOAD_METHOD             20 (c_proj)\n",
      "            322 LOAD_FAST                8 (y)\n",
      "            324 CALL_METHOD              1\n",
      "            326 CALL_METHOD              1\n",
      "            328 STORE_FAST               8 (y)\n",
      "\n",
      " 48         330 LOAD_FAST                8 (y)\n",
      "            332 RETURN_VALUE\n",
      "\n",
      " ========== TorchDynamo Stack Trace ==========\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 670, in call_user_compiler\n",
      "    compiled_fn = compiler_fn(gm, self.fake_example_inputs())\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_dynamo/debug_utils.py\", line 1055, in debug_wrapper\n",
      "    compiled_gm = compiler_fn(gm, example_inputs)\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/__init__.py\", line 1390, in __call__\n",
      "    return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 455, in compile_fx\n",
      "    return aot_autograd(\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_dynamo/backends/common.py\", line 48, in compiler_fn\n",
      "    cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 2805, in aot_module_simplified\n",
      "    compiled_fn = create_aot_dispatcher_function(\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 163, in time_wrapper\n",
      "    r = func(*args, **kwargs)\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 2498, in create_aot_dispatcher_function\n",
      "    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config)\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 1713, in aot_wrapper_dedupe\n",
      "    return compiler_fn(flat_fn, leaf_flat_args, aot_config)\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 2133, in aot_dispatch_autograd\n",
      "    compiled_fw_func = aot_config.fw_compiler(\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 163, in time_wrapper\n",
      "    r = func(*args, **kwargs)\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 430, in fw_compiler\n",
      "    return inner_compile(\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_dynamo/debug_utils.py\", line 595, in debug_wrapper\n",
      "    compiled_fn = compiler_fn(gm, example_inputs)\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_inductor/debug.py\", line 239, in inner\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 177, in compile_fx_inner\n",
      "    compiled_fn = graph.compile_to_fn()\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 586, in compile_to_fn\n",
      "    return self.compile_to_module().call\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 163, in time_wrapper\n",
      "    r = func(*args, **kwargs)\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 571, in compile_to_module\n",
      "    code = self.codegen()\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 520, in codegen\n",
      "    self.scheduler = Scheduler(self.buffers)\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 163, in time_wrapper\n",
      "    r = func(*args, **kwargs)\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 605, in __init__\n",
      "    group_fn = self.get_backend(node.get_device()).group_fn\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1131, in get_backend\n",
      "    self.backends[device] = self.create_backend(device)\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1116, in create_backend\n",
      "    device_props = torch.cuda.get_device_properties(device)\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 395, in get_device_properties\n",
      "    _lazy_init()  # will define _get_device_properties\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 239, in _lazy_init\n",
      "    raise AssertionError(\"Torch not compiled with CUDA enabled\")\n",
      "AssertionError: Torch not compiled with CUDA enabled\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 324, in _compile\n",
      "    out_code = transform_code_object(code, transform)\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 445, in transform_code_object\n",
      "    transformations(instructions, code_options)\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 311, in transform\n",
      "    tracer.run()\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1726, in run\n",
      "    super().run()\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 576, in run\n",
      "    and self.step()\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 540, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1792, in RETURN_VALUE\n",
      "    self.output.compile_subgraph(\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 517, in compile_subgraph\n",
      "    self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 588, in compile_and_call_fx_graph\n",
      "    compiled_fn = self.call_user_compiler(gm)\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 163, in time_wrapper\n",
      "    r = func(*args, **kwargs)\n",
      "  File \"/Users/cs/opt/anaconda3/envs/torch2x/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 675, in call_user_compiler\n",
      "    raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
      "torch._dynamo.exc.BackendCompilerFailed: debug_wrapper raised AssertionError: Torch not compiled with CUDA enabled\n",
      "\n",
      "\n",
      "[W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">compiled model 运行时间： <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6786.322</span> microseconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "compiled model 运行时间： \u001b[1;36m6786.322\u001b[0m microseconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch._dynamo.config.suppress_errors = True\n",
    "torch._dynamo.config.verbose=True\n",
    "\n",
    "compiled_model = torch.compile(model)\n",
    "compiled_model(x)\n",
    "print(f\"compiled model 运行时间： {torch_timer(compiled_model, x):.3f} microseconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "activities = [ProfilerActivity.CPU]\n",
    "if device == 'cuda':\n",
    "    activities.append(ProfilerActivity.CUDA)\n",
    "\n",
    "with profile(activities=activities, record_shapes=False) as prof:\n",
    "    with record_function(\" Non-Compilied Causal Attention\"):\n",
    "        for _ in range(25):\n",
    "            model(x)\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "\n",
    "with profile(activities=activities, record_shapes=False) as prof:\n",
    "    with record_function(\"Compiled Causal Attention\"):\n",
    "        for _ in range(25):\n",
    "            compiled_model(x)\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "# For even more insights, you can export the trace and use ``chrome://tracing`` to view the results\n",
    "# prof.export_chrome_trace(\"compiled_causal_attention_trace.json\")."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
