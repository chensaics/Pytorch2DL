{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from rich import print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows和Linux上使用GPU\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Mac 上使用 GPU加速：\n",
    "# device = torch.device(\"mps\")\n",
    "device = \"mps\" if torch.backends.mps.is_built() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小的示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query, key, value = torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device)\n",
    "# print(F.scaled_dot_product_attention(query, key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计时器:\n",
    "import torch.utils.benchmark as benchmark\n",
    "def torch_timer(f, *args, **kwargs):\n",
    "    t0 = benchmark.Timer(\n",
    "        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}\n",
    "    )\n",
    "    return t0.blocked_autorange().mean * 1e6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数定义\n",
    "batch_size = 64\n",
    "max_sequence_len = 256\n",
    "num_heads = 32\n",
    "embed_dimension = 32\n",
    "dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟 q k v\n",
    "query = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\n",
    "key = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\n",
    "value = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">基本对照方案 运行时间： <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17542.618</span> microseconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "基本对照方案 运行时间： \u001b[1;36m17542.618\u001b[0m microseconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"基本对照方案 运行时间： {torch_timer(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "性能对照测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.backends.cuda import sdp_kernel\n",
    "from enum import IntEnum\n",
    "\n",
    "class SDPBackend(IntEnum):\n",
    "    r\"\"\"\n",
    "    Enum class for the scaled dot product attention backends.\n",
    "    \"\"\"\n",
    "    ERROR = -1\n",
    "    MATH = 0\n",
    "    FLASH_ATTENTION = 1\n",
    "    EFFICIENT_ATTENTION = 2\n",
    "\n",
    "# 使用上下文管理器context manager来\n",
    "# 其他三种方案，字典映射\n",
    "backend_map = {\n",
    "    SDPBackend.MATH: {\n",
    "        \"enable_math\": True, \n",
    "        \"enable_flash\": False, \n",
    "        \"enable_mem_efficient\": False},\n",
    "    SDPBackend.FLASH_ATTENTION: {\n",
    "        \"enable_math\": False, \n",
    "        \"enable_flash\": True, \n",
    "        \"enable_mem_efficient\": False},\n",
    "    SDPBackend.EFFICIENT_ATTENTION: {\n",
    "        \"enable_math\": False, \n",
    "        \"enable_flash\": False, \n",
    "        \"enable_mem_efficient\": True}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">math 运行时间： <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18869.076</span> microseconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "math 运行时间： \u001b[1;36m18869.076\u001b[0m microseconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with sdp_kernel(**backend_map[SDPBackend.MATH]):\n",
    "    print(f\"math 运行时间： {torch_timer(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">flash attention 运行时间： <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">42313.492</span> microseconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "flash attention 运行时间： \u001b[1;36m42313.492\u001b[0m microseconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with sdp_kernel(**backend_map[SDPBackend.FLASH_ATTENTION]):\n",
    "    try:\n",
    "        print(f\"flash attention 运行时间： {torch_timer(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\")\n",
    "    except RuntimeError:\n",
    "        print(\"FlashAttention is not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Memory efficient 运行时间： <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">42347.333</span> microseconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Memory efficient 运行时间： \u001b[1;36m42347.333\u001b[0m microseconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n",
    "    try:\n",
    "        print(f\"Memory efficient 运行时间： {torch_timer(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\")\n",
    "    except RuntimeError:\n",
    "        print(\"EfficientAttention is not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
